{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from acquire import *\n",
    "from prepare import *\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_confusing_matrix(x, y):\n",
    "    cm = pd.DataFrame(confusion_matrix(x, y), columns=['Actual +', 'Actual -'], index=['Pred +', 'Pred -'])\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "      <th>embarked_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_id  survived  pclass     sex   age  sibsp  parch     fare  \\\n",
       "0             0         0       3    male  22.0      1      0   7.2500   \n",
       "1             1         1       1  female  38.0      1      0  71.2833   \n",
       "2             2         1       3  female  26.0      0      0   7.9250   \n",
       "3             3         1       1  female  35.0      1      0  53.1000   \n",
       "4             4         0       3    male  35.0      0      0   8.0500   \n",
       "\n",
       "  embarked  class  embark_town  alone  embarked_encode  \n",
       "0        S  Third  Southampton      0                3  \n",
       "1        C  First    Cherbourg      0                0  \n",
       "2        S  Third  Southampton      1                3  \n",
       "3        S  First  Southampton      0                3  \n",
       "4        S  Third  Southampton      1                3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep_titanic(get_titanic_data())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['age', 'fare']])\n",
    "\n",
    "train[['age', 'fare']] = scaler.transform(train[['age', 'fare']])\n",
    "test[['age', 'fare']] = scaler.transform(test[['age', 'fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "\n",
    "logit.fit(train[['pclass', 'sibsp', 'parch']], train.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.predict(train[['pclass','sibsp','parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74411695, 0.25588305],\n",
       "       [0.55874994, 0.44125006],\n",
       "       [0.74411695, 0.25588305],\n",
       "       ...,\n",
       "       [0.41040989, 0.58959011],\n",
       "       [0.54238193, 0.45761807],\n",
       "       [0.80220314, 0.19779686]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.predict_proba(train[['pclass','sibsp','parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prediction'] = logit.predict(train[['pclass', 'sibsp','parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6785046728971963"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.score(train[['pclass','sibsp','parch']], train.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[252 113]\n",
      " [ 59 111]]\n"
     ]
    }
   ],
   "source": [
    "co_m = confusion_matrix(train.survived, train.prediction).transpose()\n",
    "print(co_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.81      0.75       311\n",
      "           1       0.65      0.50      0.56       224\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       535\n",
      "   macro avg       0.67      0.65      0.65       535\n",
      "weighted avg       0.67      0.68      0.67       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train.survived, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred -</th>\n",
       "      <th>Pred +</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual -</th>\n",
       "      <td>252</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual +</th>\n",
       "      <td>113</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred -  Pred +\n",
       "Actual -     252      59\n",
       "Actual +     113     111"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(train.survived, train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_helper(cm):\n",
    "    tp = cm.loc['Actual +', 'Pred +']\n",
    "    fn = cm.loc['Actual +', 'Pred -']\n",
    "    fp = cm.loc['Actual -', 'Pred +']\n",
    "    tn = cm.loc['Actual -', 'Pred -']\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = (recall + precision) / 2\n",
    "    support = tp + fn\n",
    "\n",
    "    print(f'TP:    {tp}')\n",
    "    print(f'FN:    {fn}')\n",
    "    print(f'FP:    {fp}')\n",
    "    print(f'TN:    {fn}\\n\\n')\n",
    "\n",
    "    print(f'Accuracy: {(tp + tn) / (tp + fn + fp + tn):.2f}')\n",
    "    print(f'TP Rate:  {tp / (tp + fn):.2f} --> Recall or Sensitivity')\n",
    "    print(f'FP Rate:  {fp / (fp + tn):.2f} --> Classification')\n",
    "    print(f'TN Rate:  {tn / (fp + tn):.2f}')\n",
    "    print(f'FN Rate:  {fn / (tp + fn):.2f}\\n\\n')\n",
    "\n",
    "    print('recall:    %.2f' % recall)\n",
    "    print('precision: %.2f' % precision)\n",
    "    print('f1:        %.2f' % f1)\n",
    "    print('support:   %4d' % support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual +</th>\n",
       "      <th>Actual -</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pred +</th>\n",
       "      <td>252</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pred -</th>\n",
       "      <td>113</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual +  Actual -\n",
       "Pred +       252        59\n",
       "Pred -       113       111"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_confusing_matrix(train.survived, train.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:    111\n",
      "FN:    113\n",
      "FP:    59\n",
      "TN:    113\n",
      "\n",
      "\n",
      "Accuracy: 0.68\n",
      "TP Rate:  0.50 --> Recall or Sensitivity\n",
      "FP Rate:  0.19 --> Classification\n",
      "TN Rate:  0.81\n",
      "FN Rate:  0.50\n",
      "\n",
      "\n",
      "recall:    0.50\n",
      "precision: 0.65\n",
      "f1:        0.57\n",
      "support:    224\n"
     ]
    }
   ],
   "source": [
    "matrix_helper(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass   age      fare  sibsp  parch\n",
       "60        3  22.0    7.2292      0      0\n",
       "348       3   3.0   15.9000      1      1\n",
       "606       3  30.0    7.8958      0      0\n",
       "195       1  58.0  146.5208      0      0\n",
       "56        2  21.0   10.5000      0      0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-0.05547772 -0.0068258   0.01878062 -0.03752115  0.02680324]]\n",
      "Intercept: \n",
      " [0.00653649]\n"
     ]
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "print('Coefficient: \\n', logit.coef_)\n",
    "print('Intercept: \\n', logit.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logit.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.66\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188 105]\n",
      " [ 65 141]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual +</th>\n",
       "      <th>Actual -</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pred +</th>\n",
       "      <td>188</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pred -</th>\n",
       "      <td>65</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual +  Actual -\n",
       "Pred +       188       105\n",
       "Pred -        65       141"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_confusing_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69       293\n",
      "           1       0.57      0.68      0.62       206\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       499\n",
      "   macro avg       0.66      0.66      0.66       499\n",
      "weighted avg       0.67      0.66      0.66       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on test set: {:.2f}'\n",
    "     .format(logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a19cf11d0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_proba = [i[1] for i in y_pred_proba]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(y_pred_proba, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40789396738583267,\n",
       " 0.5837956176122896,\n",
       " 0.3483556432920513,\n",
       " 0.7531395421864524,\n",
       " 0.6748610381566311,\n",
       " 0.4318919119558114,\n",
       " 0.4007495254561765,\n",
       " 0.8213824641858191,\n",
       " 0.8271523304904407,\n",
       " 0.41558379982056814,\n",
       " 0.8458346461243514,\n",
       " 0.4318766104478761,\n",
       " 0.8666928870647661,\n",
       " 0.4356571320734681,\n",
       " 0.6125708137212633,\n",
       " 0.4168218965561323,\n",
       " 0.5710031244030409,\n",
       " 0.3763986180131791,\n",
       " 0.35875763972199964,\n",
       " 0.9511619340262736,\n",
       " 0.7432581775461992,\n",
       " 0.6110035971487222,\n",
       " 0.45305513894428173,\n",
       " 0.4008134029794431,\n",
       " 0.40863892074412317,\n",
       " 0.393034379233928,\n",
       " 0.6483047368726315,\n",
       " 0.669638860439041,\n",
       " 0.950820391439876,\n",
       " 0.34780509742297755,\n",
       " 0.43151187487983295,\n",
       " 0.724508085773407,\n",
       " 0.49954334756335184,\n",
       " 0.7336536868872493,\n",
       " 0.4483769212364834,\n",
       " 0.8014039025875441,\n",
       " 0.3583336536910362,\n",
       " 0.7237490275895759,\n",
       " 0.9548325802955278,\n",
       " 0.6555686872088434,\n",
       " 0.7578998898188966,\n",
       " 0.32688388123453843,\n",
       " 0.3193498368841352,\n",
       " 0.32821132454371377,\n",
       " 0.5801726849497347,\n",
       " 0.32922577281919163,\n",
       " 0.39895657702763504,\n",
       " 0.8628754525526975,\n",
       " 0.3360578700967044,\n",
       " 0.4995494251881191,\n",
       " 0.4846430406081932,\n",
       " 0.4317805641939534,\n",
       " 0.27910183192243826,\n",
       " 0.41621815095646686,\n",
       " 0.6963652462402276,\n",
       " 0.23527474216062258,\n",
       " 0.9466278054398207,\n",
       " 0.36442181127304585,\n",
       " 0.8333444181249419,\n",
       " 0.7764925698645021,\n",
       " 0.5476656748119405,\n",
       " 0.8031885902945679,\n",
       " 0.4563508977160666,\n",
       " 0.42049593567714133,\n",
       " 0.27095995312110094,\n",
       " 0.4664295951104859,\n",
       " 0.41026269390754944,\n",
       " 0.7276061322321256,\n",
       " 0.368047285681455,\n",
       " 0.34102238183784284,\n",
       " 0.6033684184792536,\n",
       " 0.37582894686230744,\n",
       " 0.2920282977219026,\n",
       " 0.40198689243028224,\n",
       " 0.9018252691989215,\n",
       " 0.7466606450073673,\n",
       " 0.43870280672182405,\n",
       " 0.8714398849420182,\n",
       " 0.5557850143681395,\n",
       " 0.9333624808376789,\n",
       " 0.4308360458965634,\n",
       " 0.8682533525300369,\n",
       " 0.7440415761157515,\n",
       " 0.5113044988365868,\n",
       " 0.6163663083590035,\n",
       " 0.968027658441298,\n",
       " 0.5748854885942137,\n",
       " 0.21450846466039325,\n",
       " 0.3597977022087351,\n",
       " 0.3475181624981328,\n",
       " 0.4084989375856492,\n",
       " 0.357025365014685,\n",
       " 0.5528661942194214,\n",
       " 0.7627383911707233,\n",
       " 0.7767221186604019,\n",
       " 0.9026461770163837,\n",
       " 0.4529596787033933,\n",
       " 0.45063348533374326,\n",
       " 0.43203405626778163,\n",
       " 0.8561061109180114,\n",
       " 0.9465023868620868,\n",
       " 0.2882775065183773,\n",
       " 0.8018985302298217,\n",
       " 0.843422385878283,\n",
       " 0.662760650937308,\n",
       " 0.8238544253967112,\n",
       " 0.7212153600177688,\n",
       " 0.45682377096327825,\n",
       " 0.5866932537447398,\n",
       " 0.27960916503571037,\n",
       " 0.8400659510307211,\n",
       " 0.8442576044847401,\n",
       " 0.7694530132925518,\n",
       " 0.4163968955778691,\n",
       " 0.5645028314298153,\n",
       " 0.5871323879271698,\n",
       " 0.32359159231689255,\n",
       " 0.5113044988365868,\n",
       " 0.8477075180590559,\n",
       " 0.6975554472472731,\n",
       " 0.40815857358388935,\n",
       " 0.8636123395835084,\n",
       " 0.7551959516299015,\n",
       " 0.745529217623169,\n",
       " 0.5358984112860299,\n",
       " 0.43203405626778163,\n",
       " 0.8768235720502708,\n",
       " 0.6410532800910661,\n",
       " 0.4379895324473439,\n",
       " 0.32678403349215845,\n",
       " 0.334356394383997,\n",
       " 0.5632282584354398,\n",
       " 0.7239656889243828,\n",
       " 0.7280681265744884,\n",
       " 0.8205467380822351,\n",
       " 0.9512769911885055,\n",
       " 0.6534448823835954,\n",
       " 0.3997879029683944,\n",
       " 0.37572860018357057,\n",
       " 0.42401684428863123,\n",
       " 0.5487030587843955,\n",
       " 0.34114903197068336,\n",
       " 0.6468779391480552,\n",
       " 0.8125953808686487,\n",
       " 0.3967873036365341,\n",
       " 0.3779579858852326,\n",
       " 0.43965124938314964,\n",
       " 0.49199369615008093,\n",
       " 0.865430591772176,\n",
       " 0.6110035971487222,\n",
       " 0.3724818777106404,\n",
       " 0.6230145906688032,\n",
       " 0.4083665936173533,\n",
       " 0.33963400101744806,\n",
       " 0.34848715909760375,\n",
       " 0.5828879151834989,\n",
       " 0.31873636884878725,\n",
       " 0.6839843244618836,\n",
       " 0.6084088333978971,\n",
       " 0.6732166137700861,\n",
       " 0.6778999605474153,\n",
       " 0.9618010103837031,\n",
       " 0.6847298075616324,\n",
       " 0.3704615657026069,\n",
       " 0.4396705004165541,\n",
       " 0.247504397618387,\n",
       " 0.8257589422146802,\n",
       " 0.34081788443082134,\n",
       " 0.47047552311685703,\n",
       " 0.6634590178049783,\n",
       " 0.8395989643238106,\n",
       " 0.8418352749020227,\n",
       " 0.1893882972841579,\n",
       " 0.3200055813181834,\n",
       " 0.29957293018663844,\n",
       " 0.5228242176814926,\n",
       " 0.6685292306447495,\n",
       " 0.7658075698647583,\n",
       " 0.4078901565751622,\n",
       " 0.8937392709890993,\n",
       " 0.6146685382306025,\n",
       " 0.25143825109007184,\n",
       " 0.6261084847356928,\n",
       " 0.4435433219858722,\n",
       " 0.39094386960092115,\n",
       " 0.7916898567053599,\n",
       " 0.7466606450073673,\n",
       " 0.25013237703556584,\n",
       " 0.31295549833533276,\n",
       " 0.662760650937308,\n",
       " 0.902663702117454,\n",
       " 0.35703976742648547,\n",
       " 0.41616867599954993,\n",
       " 0.3853760712697468,\n",
       " 0.33959944107265727,\n",
       " 0.42584520508619284,\n",
       " 0.3338444606576351,\n",
       " 0.38762504022074656,\n",
       " 0.9479357936828666,\n",
       " 0.5594734809971931,\n",
       " 0.8023436281746346,\n",
       " 0.6969960834525035,\n",
       " 0.8193402873898589,\n",
       " 0.3006913576762021,\n",
       " 0.4162827813026024,\n",
       " 0.6808936792433861,\n",
       " 0.38575745355914487,\n",
       " 0.9090923669436953,\n",
       " 0.2998496579068773,\n",
       " 0.737646573259228,\n",
       " 0.3203862095289074,\n",
       " 0.33396628632632985,\n",
       " 0.3941840292671042,\n",
       " 0.5918216789387969,\n",
       " 0.3295296368339792,\n",
       " 0.662760650937308,\n",
       " 0.6588718000936798,\n",
       " 0.18741788811115612,\n",
       " 0.7050851940062514,\n",
       " 0.7006794579701207,\n",
       " 0.7621881096910539,\n",
       " 0.6483047368726315,\n",
       " 0.46366796068948585,\n",
       " 0.3055967312658429,\n",
       " 0.8310074290467995,\n",
       " 0.52864563801161,\n",
       " 0.4933505640961649,\n",
       " 0.3340951548702065,\n",
       " 0.8416761058933944,\n",
       " 0.7048782986550991,\n",
       " 0.7657088665378539,\n",
       " 0.4160317614954328,\n",
       " 0.3200055813181834,\n",
       " 0.31227269022086684,\n",
       " 0.219951698283064,\n",
       " 0.4161458560166192,\n",
       " 0.5934184923993863,\n",
       " 0.6555686872088434,\n",
       " 0.37048715016969197,\n",
       " 0.351067421591442,\n",
       " 0.7883942812248921,\n",
       " 0.7386540350040418,\n",
       " 0.36965515244000297,\n",
       " 0.36645987514583694,\n",
       " 0.8124999078959757,\n",
       " 0.34486189536430495,\n",
       " 0.4469487112626078,\n",
       " 0.42327914580746195,\n",
       " 0.4169561848367165,\n",
       " 0.8288910024618132,\n",
       " 0.627019104083487,\n",
       " 0.34900978273146205,\n",
       " 0.5669244742442845,\n",
       " 0.299937666352995,\n",
       " 0.31308007044574515,\n",
       " 0.674529195092332,\n",
       " 0.3785105019154443,\n",
       " 0.3963498846615737,\n",
       " 0.8638124681054333,\n",
       " 0.2926856787842757,\n",
       " 0.34848715909760375,\n",
       " 0.4787208830272848,\n",
       " 0.3629401888013856,\n",
       " 0.6110035971487222,\n",
       " 0.45544127439924764,\n",
       " 0.6907592880322968,\n",
       " 0.39312400523630336,\n",
       " 0.9669401114733917,\n",
       " 0.32751109811193696,\n",
       " 0.37037395317436783,\n",
       " 0.6110035971487222,\n",
       " 0.34108925028144194,\n",
       " 0.9329978402839579,\n",
       " 0.8053373754481503,\n",
       " 0.8328613777892232,\n",
       " 0.44857386664252624,\n",
       " 0.8771607405030154,\n",
       " 0.344607303046092,\n",
       " 0.4095016592617891,\n",
       " 0.9351910219052325,\n",
       " 0.356191120039102,\n",
       " 0.7583153619134417,\n",
       " 0.4161077930849341,\n",
       " 0.8346596232329883,\n",
       " 0.31308007044574515,\n",
       " 0.43761030937086687,\n",
       " 0.3416477951684278,\n",
       " 0.5168389941904452,\n",
       " 0.3339906875466849,\n",
       " 0.45376820602026213,\n",
       " 0.42173807211831715,\n",
       " 0.5168389941904452,\n",
       " 0.4933059133396647,\n",
       " 0.42401684428863123,\n",
       " 0.3454217138975093,\n",
       " 0.3855132060457357,\n",
       " 0.39236617605983026,\n",
       " 0.3614934913198442,\n",
       " 0.46446581893522115,\n",
       " 0.8490751583619015,\n",
       " 0.14839261525413694,\n",
       " 0.4557363749972493,\n",
       " 0.6312960281913225,\n",
       " 0.31540344443288454,\n",
       " 0.9574675672514641,\n",
       " 0.6444632428217797,\n",
       " 0.5742770172797621,\n",
       " 0.6963652462402276,\n",
       " 0.4415649212108027,\n",
       " 0.2987360018589476,\n",
       " 0.6337032821616478,\n",
       " 0.43965124938314964,\n",
       " 0.6166464723032561,\n",
       " 0.4240818059121255,\n",
       " 0.6603191950015358,\n",
       " 0.535412962201863,\n",
       " 0.538768421975674,\n",
       " 0.8564720823414815,\n",
       " 0.5829239585467618,\n",
       " 0.3557902235891811,\n",
       " 0.6163663083590035,\n",
       " 0.43962811136211305,\n",
       " 0.4083665936173533,\n",
       " 0.3013919050212911,\n",
       " 0.8974179829550238,\n",
       " 0.7135285619304103,\n",
       " 0.6261084847356928,\n",
       " 0.5636737824105404,\n",
       " 0.6896562838991923,\n",
       " 0.8714398849420182,\n",
       " 0.6310366318801767,\n",
       " 0.2674603862496203,\n",
       " 0.7970581539203025,\n",
       " 0.38344426351908817,\n",
       " 0.6738298484737781,\n",
       " 0.6782411696109889,\n",
       " 0.5511427001820856,\n",
       " 0.7397986333691969,\n",
       " 0.2375169916195103,\n",
       " 0.3339906875466849,\n",
       " 0.25466739106944786,\n",
       " 0.7637252161977158,\n",
       " 0.5401794390216759,\n",
       " 0.3778512948169022,\n",
       " 0.3779506549120678,\n",
       " 0.67582309946552,\n",
       " 0.31540344443288454,\n",
       " 0.43631866934278596,\n",
       " 0.38476074657436976,\n",
       " 0.3787229883238194,\n",
       " 0.8478366075771585,\n",
       " 0.3346901563389613,\n",
       " 0.4083892853985365,\n",
       " 0.9078196259972995,\n",
       " 0.4079128401352632,\n",
       " 0.9083027644813276,\n",
       " 0.45456055024722686,\n",
       " 0.3779579858852326,\n",
       " 0.724381567833124,\n",
       " 0.8402408719107565,\n",
       " 0.6769085175252316,\n",
       " 0.25466739106944786,\n",
       " 0.4788713390534374,\n",
       " 0.33249140363994006,\n",
       " 0.16556871386543204,\n",
       " 0.2596342223769223,\n",
       " 0.5474691674468896,\n",
       " 0.6725090921057862,\n",
       " 0.515997005812401,\n",
       " 0.3945627656788424,\n",
       " 0.3852944753697959,\n",
       " 0.2202555497729049,\n",
       " 0.8802395624015813,\n",
       " 0.66342510817856,\n",
       " 0.7796777694866333,\n",
       " 0.8851688756237803,\n",
       " 0.7086435528741335,\n",
       " 0.49704250055693605,\n",
       " 0.7067237537519331,\n",
       " 0.4999961636537167,\n",
       " 0.4570954387438586,\n",
       " 0.3526953141656255,\n",
       " 0.3780941931454039,\n",
       " 0.625800621984415,\n",
       " 0.3522709768586417,\n",
       " 0.31524433673727303,\n",
       " 0.15758130034422918,\n",
       " 0.2861250620248445,\n",
       " 0.6155198599643144,\n",
       " 0.957263858646231,\n",
       " 0.5716176540903876,\n",
       " 0.36454423304840106,\n",
       " 0.8584722253008246,\n",
       " 0.6764422137304722,\n",
       " 0.8184337541239121,\n",
       " 0.39801278542083746,\n",
       " 0.7555461601123845,\n",
       " 0.49466384805010055,\n",
       " 0.28320088858276393,\n",
       " 0.42376420933162906,\n",
       " 0.5872894316015577,\n",
       " 0.7249689057657812,\n",
       " 0.9033023882681656,\n",
       " 0.42527506823758415,\n",
       " 0.6448344063860755,\n",
       " 0.677939202974019,\n",
       " 0.3854057036552633,\n",
       " 0.31233320430342765,\n",
       " 0.4164139832468262,\n",
       " 0.40863892074412317,\n",
       " 0.924552357021375,\n",
       " 0.16453078330670007,\n",
       " 0.24842838492036057,\n",
       " 0.6104230437444224,\n",
       " 0.7671771001695509,\n",
       " 0.4255276012095301,\n",
       " 0.30705799454136995,\n",
       " 0.23100266673774622,\n",
       " 0.6810602925378785,\n",
       " 0.4162827813026024,\n",
       " 0.9245353953541451,\n",
       " 0.41674687140837824,\n",
       " 0.7010934633018605,\n",
       " 0.31391116924009854,\n",
       " 0.8734649113314625,\n",
       " 0.33489082187334257,\n",
       " 0.4004751070400114,\n",
       " 0.704480896263752,\n",
       " 0.7885750527613126,\n",
       " 0.7481304886172918,\n",
       " 0.632788666474363,\n",
       " 0.9726826509155334,\n",
       " 0.966374989586035,\n",
       " 0.35555346201299237,\n",
       " 0.9162105893480428,\n",
       " 0.535375288344025,\n",
       " 0.8073713623257851,\n",
       " 0.8784995939522783,\n",
       " 0.9429879110678322,\n",
       " 0.5365096199132706,\n",
       " 0.8076715162592845,\n",
       " 0.4688667829657104,\n",
       " 0.845806394847332,\n",
       " 0.5057204554741603,\n",
       " 0.26664231706715263,\n",
       " 0.6555686872088434,\n",
       " 0.7089556214467329,\n",
       " 0.2486388780348584,\n",
       " 0.7837665148664051,\n",
       " 0.7754775687485119,\n",
       " 0.4163968955778691,\n",
       " 0.45063348533374326,\n",
       " 0.8097737072694148,\n",
       " 0.4108801974747669,\n",
       " 0.7785377700524377,\n",
       " 0.3310864401753618,\n",
       " 0.5778833261097244,\n",
       " 0.7208230937026917,\n",
       " 0.5621925047037083,\n",
       " 0.8019801320353963,\n",
       " 0.774697542287582,\n",
       " 0.28786681777918843,\n",
       " 0.41440385489961307,\n",
       " 0.4062070452170122,\n",
       " 0.6087689903149297,\n",
       " 0.6317046753639528,\n",
       " 0.30859582267987007,\n",
       " 0.34100127568802135,\n",
       " 0.6335717618607491,\n",
       " 0.5557850143681395,\n",
       " 0.46863791169389923,\n",
       " 0.377983777222192,\n",
       " 0.3584120953886543,\n",
       " 0.39326222263634547,\n",
       " 0.9618010103837031,\n",
       " 0.4234434977698689,\n",
       " 0.6063304187228247,\n",
       " 0.5948702540962104,\n",
       " 0.38519014847628025,\n",
       " 0.3927655444665525,\n",
       " 0.8755494211104508,\n",
       " 0.33856699564957876,\n",
       " 0.43399017351056113,\n",
       " 0.17406794383083818,\n",
       " 0.7283816773908935,\n",
       " 0.6317046753639528,\n",
       " 0.32901839492348767,\n",
       " 0.531787594127679,\n",
       " 0.28631326633362036,\n",
       " 0.8869540434252046,\n",
       " 0.19123316551560873,\n",
       " 0.6483047368726315,\n",
       " 0.8461399459773924,\n",
       " 0.5842654708517937,\n",
       " 0.7960731483365135,\n",
       " 0.2861250620248445,\n",
       " 0.6604111198910073,\n",
       " 0.3556574562203757,\n",
       " 0.7037139683641916]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = prep_iris(get_iris_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.columns = [col.lower().replace('.', '_') for col in dfi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  species_encode\n",
       "114           5.8          2.8           5.1          2.4               2\n",
       "136           6.3          3.4           5.6          2.4               2\n",
       "53            5.5          2.3           4.0          1.3               1\n",
       "19            5.1          3.8           1.5          0.3               0\n",
       "38            4.4          3.0           1.3          0.2               0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dfi.drop(['species'],axis=1)\n",
    "y = dfi[['species']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica', 'virginica', 'versicolor', 'setosa', 'setosa'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32,  0,  0],\n",
       "       [ 0, 40,  0],\n",
       "       [ 0,  0, 33]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    40\n",
       "virginica     33\n",
       "setosa        32\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          32           0          0\n",
       "versicolor       0          40          0\n",
       "virginica        0           0         33"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_train.species.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       1.00      1.00      1.00        40\n",
      "   virginica       1.00      1.00      1.00        33\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       105\n",
      "   macro avg       1.00      1.00      1.00       105\n",
      "weighted avg       1.00      1.00      1.00       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on test set: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree.png'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data, filename='tree', format='png') \n",
    "graph.render(cleanup=True, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
