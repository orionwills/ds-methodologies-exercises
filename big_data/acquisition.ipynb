{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Spark\" data-toc-modified-id=\"Spark-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Spark</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:19:29.044436Z",
     "start_time": "2019-05-14T21:19:27.140599Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# spark\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import rand\n",
    "import pyspark\n",
    "import multiprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:19:35.661784Z",
     "start_time": "2019-05-14T21:19:31.149540Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"read\").\\\n",
    "    enableHiveSupport().\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read into spark environment (`df_case`, `df_dept`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:19:49.212551Z",
     "start_time": "2019-05-14T21:19:35.665979Z"
    }
   },
   "outputs": [],
   "source": [
    "df_case = spark.read.format(\"csv\").\\\n",
    "    option(\"sep\", \",\").\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/case.csv\")\n",
    "\n",
    "df_dept = spark.read.format(\"csv\").\\\n",
    "    option(\"sep\", \",\").\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/dept.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write `df_case` and `df_dept` back to disk into their own directories (`my_cases` and `my_depts`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:20:00.890301Z",
     "start_time": "2019-05-14T21:19:49.216007Z"
    }
   },
   "outputs": [],
   "source": [
    "df_case.write.format('csv').mode('overwrite').option('header', True).save('sa311/my_cases.csv')\n",
    "df_dept.write.format('csv').mode('overwrite').option('header', True).save('sa311/my_depts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write `df_case` and `df_dept` to parquet files (`my_cases_parquet` and `my_depts_parquet`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:20:15.173075Z",
     "start_time": "2019-05-14T21:20:00.895148Z"
    }
   },
   "outputs": [],
   "source": [
    "df_case.write.format('parquet').mode('overwrite').option('header', True).mode(\n",
    "    'overwrite').save('sa311/my_cases_parquet')\n",
    "df_dept.write.format('parquet').mode('overwrite').option('header', True).mode(\n",
    "    'overwrite').save('sa311/my_depts_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read your parquet files back into your spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:20:15.761747Z",
     "start_time": "2019-05-14T21:20:15.177395Z"
    }
   },
   "outputs": [],
   "source": [
    "df_case = spark.read.format('parquet').\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/my_cases_parquet\")\n",
    "\n",
    "df_dept = spark.read.format('parquet').\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/my_depts_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Read `case.csv` and `dept.csv` into a pandas dataframe. (`cases_pdf`, `depts_pdf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:20:20.778485Z",
     "start_time": "2019-05-14T21:20:15.764947Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases_pdf = pd.read_csv('sa311/case.csv')\n",
    "depts_pdf = pd.read_csv('sa311/dept.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Convert the pandas dataframes into spark dataframes (`cases_sdf`, `depts_sdf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:13:39.515279Z",
     "start_time": "2019-05-14T21:13:32.908Z"
    }
   },
   "outputs": [],
   "source": [
    "depts_pdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:13:39.517612Z",
     "start_time": "2019-05-14T21:13:33.995Z"
    }
   },
   "outputs": [],
   "source": [
    "depts_pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:15:31.426800Z",
     "start_time": "2019-05-14T21:15:31.276921Z"
    }
   },
   "outputs": [],
   "source": [
    "cases_pdf.case_closed_date.fillna('na', inplace=True)\n",
    "cases_pdf.SLA_due_date.fillna('na', inplace=True)\n",
    "cases_pdf.num_days_late.fillna(0.0, inplace=True)\n",
    "cases_pdf.SLA_days.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:15:32.336807Z",
     "start_time": "2019-05-14T21:15:32.331479Z"
    }
   },
   "outputs": [],
   "source": [
    "depts_pdf.dept_name.fillna('na', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:17:05.582289Z",
     "start_time": "2019-05-14T21:15:33.694496Z"
    }
   },
   "outputs": [],
   "source": [
    "cases_sdf = spark.createDataFrame(cases_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-14T21:15:34.112Z"
    }
   },
   "outputs": [],
   "source": [
    "depts_sdf = spark.createDataFrame(depts_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Convert the spark dataframes back into pandas dataframes. (`cases_pdf1`, `depts_pdf1`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:21:13.321260Z",
     "start_time": "2019-05-14T21:21:12.927385Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_depts = spark.read.format(\"csv\").\\\n",
    "    option(\"sep\", \",\").\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/dept.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:21:58.889404Z",
     "start_time": "2019-05-14T21:21:52.250791Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf_cases = spark.read.format(\"csv\").\\\n",
    "    option(\"sep\", \",\").\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/case.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write the spark dataframes (`cases_sdf`, `depts_sdf`) to Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:22:11.873314Z",
     "start_time": "2019-05-14T21:22:11.868745Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:25:24.647027Z",
     "start_time": "2019-05-14T21:25:24.369773Z"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"df_\" + str(uuid.uuid4().hex)  \n",
    "sdf_depts.write.saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:25:48.445315Z",
     "start_time": "2019-05-14T21:25:40.253864Z"
    }
   },
   "outputs": [],
   "source": [
    "table_name2 = \"df_\" + str(uuid.uuid4().hex)\n",
    "sdf_cases.write.saveAsTable(table_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:25:53.996191Z",
     "start_time": "2019-05-14T21:25:53.983855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_034f4c256c6042e3aa9dd93d8e460616'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T21:25:58.329723Z",
     "start_time": "2019-05-14T21:25:58.324002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_b45958b336fe4b338c7aee316111b950'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
